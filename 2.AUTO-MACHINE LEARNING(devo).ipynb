{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://awesomeopensource.com/projects/automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AUTOMATIC MACHINE LEARNING**\n",
    "\n",
    "Check some tools to reduce the time training models.\n",
    "\n",
    "\n",
    "1. **Tools for Automatic Hyperparameter Optimization**\n",
    "2. **Tools for architecture search (deep learning)**\n",
    "3. **Tools for automatic ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************************************\n",
    "## **ARCHITECTURE SEARCH**\n",
    "\n",
    "We also provide a package for architecture search:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IMAGE CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEvol**\n",
    "\n",
    "Genetic neural architecture search with Keras.\n",
    "\n",
    "https://github.com/joeddav/devol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/erikapat/Dropbox/PRUEBAS_DATA_SCIENCE/devol-master\n",
      "Requirement already satisfied: keras in /home/erikapat/anaconda3/lib/python3.7/site-packages (from devol==0.2) (2.2.5)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/erikapat/anaconda3/lib/python3.7/site-packages (from keras->devol==0.2) (1.17.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/erikapat/anaconda3/lib/python3.7/site-packages (from keras->devol==0.2) (1.1.0)\n",
      "Requirement already satisfied: h5py in /home/erikapat/anaconda3/lib/python3.7/site-packages (from keras->devol==0.2) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/erikapat/anaconda3/lib/python3.7/site-packages (from keras->devol==0.2) (5.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/erikapat/anaconda3/lib/python3.7/site-packages (from keras->devol==0.2) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/erikapat/anaconda3/lib/python3.7/site-packages (from keras->devol==0.2) (1.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/erikapat/anaconda3/lib/python3.7/site-packages (from keras->devol==0.2) (1.13.0)\n",
      "Installing collected packages: devol\n",
      "  Found existing installation: devol 0.2\n",
      "    Uninstalling devol-0.2:\n",
      "      Successfully uninstalled devol-0.2\n",
      "  Running setup.py develop for devol\n",
      "Successfully installed devol\n"
     ]
    }
   ],
   "source": [
    "## **Installation**\n",
    "\n",
    "#To setup, just clone the repo: https://github.com/joeddav/devol and run pip install -e path/to/repo. You should then be able to access devol globally.\n",
    "\n",
    "!pip install -e /home/erikapat/Dropbox/PRUEBAS_DATA_SCIENCE/devol-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from devol import DEvol, GenomeHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "dataset = ((x_train, y_train), (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_handler = GenomeHandler(max_conv_layers=6, \n",
    "                               max_dense_layers=2, # includes final dense layer\n",
    "                               max_filters=256,\n",
    "                               max_dense_nodes=1024,\n",
    "                               input_shape=x_train.shape[1:],\n",
    "                               n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genome encoding and metric data stored at Wed Nov 13 12:18:51 2019.csv \n",
      "\n",
      "\n",
      "model 1/5 - generation 1/20:\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 1022s 19ms/step - loss: 14.5110 - acc: 0.0991 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 968s 18ms/step - loss: 14.5188 - acc: 0.0992 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 00002: early stopping\n",
      "\n",
      "model 2/5 - generation 1/20:\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 885s 16ms/step - loss: 0.6271 - acc: 0.7973 - val_loss: 0.1631 - val_acc: 0.9538\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 819s 15ms/step - loss: 0.1645 - acc: 0.9499 - val_loss: 0.7882 - val_acc: 0.7375\n",
      "Epoch 00002: early stopping\n",
      "\n",
      "model 3/5 - generation 1/20:\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 227s 4ms/step - loss: 0.3951 - acc: 0.8719 - val_loss: 0.3839 - val_acc: 0.8777\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 225s 4ms/step - loss: 0.1209 - acc: 0.9634 - val_loss: 0.2287 - val_acc: 0.9368\n",
      "Epoch 3/5\n",
      "54000/54000 [==============================] - 228s 4ms/step - loss: 0.0917 - acc: 0.9720 - val_loss: 0.4859 - val_acc: 0.8532\n",
      "Epoch 00003: early stopping\n",
      "\n",
      "model 4/5 - generation 1/20:\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 1004s 19ms/step - loss: 14.5088 - acc: 0.0994 - val_loss: 14.5708 - val_acc: 0.0960\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 1004s 19ms/step - loss: 14.5143 - acc: 0.0995 - val_loss: 14.5708 - val_acc: 0.0960\n",
      "Epoch 00002: early stopping\n",
      "\n",
      "model 5/5 - generation 1/20:\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 70s 1ms/step - loss: 14.5167 - acc: 0.0989 - val_loss: 14.5412 - val_acc: 0.0978\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 64s 1ms/step - loss: 14.5254 - acc: 0.0988 - val_loss: 14.5412 - val_acc: 0.0978\n",
      "Epoch 00002: early stopping\n",
      "Generation 1:\t\tbest accuracy: 0.8432\t\taverage:0.3696\t\tstd: 0.3323\n",
      "\n",
      "model 1/5 - generation 2/20:\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 839s 16ms/step - loss: 0.6243 - acc: 0.7973 - val_loss: 0.2162 - val_acc: 0.9360\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 849s 16ms/step - loss: 0.1563 - acc: 0.9523 - val_loss: 0.1346 - val_acc: 0.9615\n",
      "Epoch 3/5\n",
      "54000/54000 [==============================] - 864s 16ms/step - loss: 0.1125 - acc: 0.9649 - val_loss: 0.1026 - val_acc: 0.9702\n",
      "Epoch 4/5\n",
      "21536/54000 [==========>...................] - ETA: 8:12 - loss: 0.0930 - acc: 0.9720"
     ]
    }
   ],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "model = devol.run(dataset=dataset,\n",
    "                  num_generations=20,\n",
    "                  pop_size=5, #20,\n",
    "                  epochs=5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HyperAS: a combination of Keras and Hyperopt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://maxpumperla.com/hyperas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Talos: Hyperparameter Scanning and Optimization for Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AutoNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************************************************\n",
    "## **AUTOMATIC ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AutoWeka**\n",
    "It is an approach for the simultaneous selection of a machine learning algorithm and its hyperparameters; combined with the WEKA package it automatically yields good models for a wide variety of data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Auto-Sklearn Implementation**\n",
    "\n",
    "Very similar to the Auto-Keras. It is an extension of AutoWEKA using the Python library scikit-learn which is a drop-in replacement for regular scikit-learn classifiers and regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "ln: failed to create symbolic link '/usr/bin/swig': Permission denied\n"
     ]
    }
   ],
   "source": [
    "!apt-get remove swig\n",
    "!apt-get install swig3.0\n",
    "!ln -s /usr/bin/swig3.0 /usr/bin/swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO INSTALL\n",
    "#!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\n",
    "#!pip install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autosklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e54cf66eb3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautosklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autosklearn'"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we load in the dataset and split it into a training and test set. We then import the AutoSklearnClassifier from autosklearn.classification. Once this is done we fit the classifier to our dataset, make predictions and check the accuracy. That’s all you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f9a562f6398d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_digits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mautoml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautosklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoSklearnClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
    "automl = autosklearn.classification.AutoSklearnClassifier()\n",
    "automl.fit(X_train, y_train)\n",
    "y_hat = automl.predict(X_test)\n",
    "print(\"Score of Accuracy\", sklearn.metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TPOT**\n",
    "\n",
    "It is a data-science assistant which optimizes machine learning pipelines using genetic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* H2O AutoML provides automated model selection and ensembling for the H2O machine learning and data analytics platform.\n",
    "* TransmogrifAI is an AutoML library running on top of Spark.\n",
    "* MLBoX is an AutoML  library with three components: preprocessing, optimisation and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WARNING: The --force flag will be removed in a future conda release.\n",
      "         See 'conda install --help' for details about the --force-reinstall\n",
      "         and --clobber flags.\n",
      "\n",
      "\n",
      "\n",
      "CondaValueError: too few arguments, must supply command line package specs or --file\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from re import findall, MULTILINE\n",
    "from sys import stdin\n",
    "from conda.cli import main\n",
    "\n",
    "main(\"conda\", \"install\", \"-y\", \"--force\",  # Maybe add a '--force'/'--force-reinstall' (I didn't add it for the one-liner above)\n",
    "         *findall(r\"^\\s*-\\s*(\\S+)$\", stdin.read(), MULTILINE)  # Here are the offenders\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AUTOMATIC Hyperparameter optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperopt, including the TPE algorithm\n",
    "* Sequential Model-based Algorithm Configuration (SMAC)\n",
    "* Spearmint\n",
    "\n",
    "We also provide packages for hyperparameter optimization:\n",
    "\n",
    "* BOHB: Bayesian Optimization combined with HyperBand\n",
    "* RoBO – Robust Bayesian Optimization framework\n",
    "* SMAC3 – a python re-implementation of the SMAC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
